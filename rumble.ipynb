{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from requests.exceptions import URLRequired\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "qloToFXnRpoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a list of all channel URLs that appear for a given channel query\n",
        "def get_channel_list(query):\n",
        "    channel_list = []\n",
        "    url = 'https://rumble.com/search/channel?q=' + query\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    channels = soup.findAll('a', attrs={\"class\": \"channel-item--a\"})\n",
        "\n",
        "    for channel in channels:\n",
        "        channel_list.append('https://rumble.com' + channel.get('href'))\n",
        "\n",
        "    return channel_list"
      ],
      "metadata": {
        "id": "Hx1i9wFvRszO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a list of all video URLs across all pages that appear for a given search query\n",
        "def get_video_list_paginator(query):\n",
        "    page_urls = []\n",
        "    base_url = 'https://rumble.com/search/all?q=' + query\n",
        "    page_urls.append(base_url)\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    pages = soup.findAll('a', attrs={\"class\": \"paginator--link\"})\n",
        "\n",
        "    for page in pages:\n",
        "        page_urls.append('https://rumble.com' + page.get('href'))\n",
        "        if (page.get('aria-label') == '»'):\n",
        "            arrow_page_url = 'https://rumble.com' + page.get('href')\n",
        "            paginator_helper(arrow_page_url, page_urls)\n",
        "\n",
        "    video_urls = []\n",
        "    for url in page_urls:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        videos = soup.findAll('a', attrs={\"class\": \"video-item--a\"})\n",
        "\n",
        "        for video in videos:\n",
        "            video_urls.append('https://rumble.com' + video.get('href'))\n",
        "\n",
        "    return video_urls"
      ],
      "metadata": {
        "id": "lHB1tb6tRzAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive helper function that returns list of all subsequent page URLs for a given page URL\n",
        "def paginator_helper(url, page_urls):\n",
        "    first_page = int(url[-1])\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    pages = soup.findAll('a', attrs={\"class\": \"paginator--link\"})\n",
        "\n",
        "    # Base case for recursive function\n",
        "    if (len(pages) == 0):\n",
        "        return page_urls\n",
        "\n",
        "    # Finds index of first page we want to look at\n",
        "    start_idx = 0\n",
        "    for i, page in enumerate(pages):\n",
        "        if (page.get('aria-label') == str(first_page + 1)):\n",
        "            start_idx = i\n",
        "            break\n",
        "\n",
        "    # If all the pages can be found on the current URL, append those pages to the list\n",
        "    if (pages[-1].get('aria-label') != '»'):\n",
        "        for page in pages[start_idx:]:\n",
        "            page_urls.append('https://rumble.com' + page.get('href'))\n",
        "        return page_urls\n",
        "\n",
        "    # Else, if there are more pages left to be found, append and recurse\n",
        "    for page in pages[start_idx:]:\n",
        "        page_urls.append('https://rumble.com' + page.get('href'))\n",
        "        if (page.get('aria-label') == '»'):\n",
        "            arrow_page_url = 'https://rumble.com' + page.get('href')\n",
        "            paginator_helper(arrow_page_url, page_urls)\n",
        "    return page_urls"
      ],
      "metadata": {
        "id": "aVMNwWGLR3Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a list of all video urls that a given channel uploaded by recency\n",
        "def videos_by_channel(channel):\n",
        "  page_urls = []\n",
        "  url = 'https://rumble.com/c/' + channel\n",
        "  page_urls.append(url)\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  pages = soup.findAll('a', attrs={\"class\": \"paginator--link\"})\n",
        "  for i in range(len(pages)):\n",
        "    page_urls.append('https://rumble.com' + pages[i].get('href'))\n",
        "    if (pages[i].get('aria-label') == '»'):\n",
        "      arrow_page_url = 'https://rumble.com' + pages[i].get('href')\n",
        "      paginator_helper(arrow_page_url, page_urls)\n",
        "\n",
        "  video_urls = []\n",
        "  for url in page_urls:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    videos = soup.findAll('a', attrs={\"class\": \"video-item--a\"})\n",
        "    for video in videos:\n",
        "      video_urls.append('https://rumble.com' + video.get('href'))\n",
        "  return video_urls"
      ],
      "metadata": {
        "id": "OcgvEK7ySAEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns integer value of upvote_count for a given video url\n",
        "def get_upvotes(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  upvote_element = soup.find('span', class_='rumbles-up-votes')\n",
        "  upvote_count = upvote_element.get_text(strip=True)\n",
        "  return upvote_count"
      ],
      "metadata": {
        "id": "ERzpON2KSFnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns integer value of downvote_count for a given video url\n",
        "def get_downvotes(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  downvote_element = soup.find('span', class_='rumbles-down-votes')\n",
        "  downvote_count = downvote_element.get_text(strip=True)\n",
        "  return downvote_count"
      ],
      "metadata": {
        "id": "cyNmqzOWSF2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a JSON of data for a given video url (Title, URL, Published Date, Channel, Channel URL, Duration, Upvotes, Downvotes, Viewcount)\n",
        "def get_video_data(url):\n",
        "    result = []\n",
        "\n",
        "    headers = {\n",
        "      #'authority': 'rumble.com',\n",
        "      'accept': '*/*',\n",
        "      'accept-language': 'en-US,en;q=0.9',\n",
        "      'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Mobile Safari/537.36',\n",
        "    }\n",
        "\n",
        "    html = requests.get(url)\n",
        "    soup = BeautifulSoup(html.text, 'html.parser')\n",
        "    text = str(soup)\n",
        "    video_id_begin = text.find('rel=\"preconnect\"/><link href=\"https://rumble.com/api/Media/oembed.json?url=https%3A%2F%2Frumble.com%2Fembed%2F') + 110\n",
        "    video_id_end = text.find('%2F\" rel=\"alternate\" title', video_id_begin)\n",
        "    video_id = text[video_id_begin:video_id_end]\n",
        "\n",
        "    params = {\n",
        "      'request': 'video',\n",
        "      'ver': '2',\n",
        "      'v': video_id,\n",
        "      'ext': '{\"ad_count\":null}',\n",
        "      'ad_wt': '339',\n",
        "    }\n",
        "\n",
        "    response = requests.get('https://rumble.com/embedJS/u3/', params=params, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text = str(soup)\n",
        "\n",
        "    begin_title = text.find('title') + 8\n",
        "    end_title = text.find(',\"author')\n",
        "    title = 'Title: ' + text[begin_title:end_title - 1]\n",
        "    result.append(title)\n",
        "\n",
        "    video_url = 'Video URL: ' + url\n",
        "    result.append(video_url)\n",
        "\n",
        "    begin_pubdate = text.find('pubDate') + 10\n",
        "    end_pubdate = text.find(',\"loaded\"')\n",
        "    pubdate = 'Published Date: ' + text[begin_pubdate:end_pubdate - 1]\n",
        "    result.append(pubdate)\n",
        "\n",
        "    begin_channel = text.find('\"author\":{\"name\":') + 18\n",
        "    end_channel = text.find(',\"url\":')\n",
        "    channel = 'Channel: ' + text[begin_channel:end_channel - 1]\n",
        "    result.append(channel)\n",
        "\n",
        "    begin_channelurl = text.find(',\"url\":') + 8\n",
        "    end_channelurl = text.find('},\"player\"')\n",
        "    channelurl = 'Channel URL: ' + text[begin_channelurl:end_channelurl - 1]\n",
        "    result.append(channelurl)\n",
        "\n",
        "    begin_duration = text.find('\"duration\":') + 11\n",
        "    end_duration = text.find(',\"pubDate\":')\n",
        "    duration = 'Duration: ' + text[begin_duration:end_duration]\n",
        "    result.append(duration)\n",
        "\n",
        "    upvotes = 'Upvotes: ' + get_upvotes(url)\n",
        "    result.append(upvotes)\n",
        "\n",
        "    downvotes = 'Downvotes: ' + get_downvotes(url)\n",
        "    result.append(downvotes)\n",
        "\n",
        "    html = requests.get(url)\n",
        "    soup = BeautifulSoup(html.text, 'html.parser')\n",
        "    text = str(soup)\n",
        "\n",
        "    video_count_class_idx = text.find('svg class=\"video-counters--icon\"')\n",
        "    a = text.find('</svg>', video_count_class_idx)\n",
        "    begin_video_viewcount = a + 6\n",
        "    end_video_viewcount = text.find('\\t', begin_video_viewcount)\n",
        "    viewcount = 'Viewcount: ' + text[begin_video_viewcount: end_video_viewcount]\n",
        "    result.append(viewcount)\n",
        "\n",
        "    # convert list to json\n",
        "    json_result = json.dumps(result)\n",
        "    return json_result"
      ],
      "metadata": {
        "id": "54hmVqHMSGE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tests\n",
        "\n",
        "channel_videos = videos_by_channel('JohnnyB')\n",
        "print(channel_videos)\n",
        "\n",
        "vid_data = get_video_data('https://rumble.com/v2fmc9h-building-a-amazing-split-flap-clock-arduino.html')\n",
        "print(vid_data)\n",
        "\n",
        "vids_urls = get_video_list_paginator('cheese cats cute')\n",
        "print(vids_urls)\n",
        "\n",
        "channels = get_channel_list('trump')\n",
        "print(channels)"
      ],
      "metadata": {
        "id": "mdj0MuI7biMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}